{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Char15Xu/nndl_final_proj/blob/main/convnext_clip.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Description:\n",
        "    This notebook implements a dual-model architecture for hierarchical multi-class image classification.\n",
        "    - ConvNeXt is used for superclass prediction\n",
        "    - CLIP (ViT-B-32 or ViT-B-16) is used for subclass prediction.\n",
        "Optimization techniques used:\n",
        "    - Data augmentation (RandomResizedCrop, RandomHorizontalFlip, etc.)\n",
        "    - Parameter reduction (Freezing CLIP visual backbone and training only a linear head)\n",
        "    - Weight decay regularization (AdamW optimizer with weight_decay=0.05)\n",
        "    - Early stopping based on validation loss (to prevent overfitting)\n",
        "    - Stochastic regularization (ConvNeXt uses DropPath)\n",
        "    - Reproducible random seed for dataset splitting\n",
        "    - (Optional) Ensemble predictions for further robustness (not enabled by default)\n",
        "\n",
        "Author: Charles Xu (tx2263)\n",
        "Date: 2025-05-13\n",
        "Credit: This notebook has adopted MultiClassImageDataset and MultiClassImageTestDataset from NNDL simple_cnn_demo file.\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "1NsEXyvIUIxZ",
        "outputId": "b9e73ce4-a43a-4587-d1ef-9d76950d8af8"
      },
      "id": "1NsEXyvIUIxZ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nDescription:\\n    This notebook implements a dual-model architecture for hierarchical multi-class image classification.\\n    - ConvNeXt is used for superclass prediction \\n    - CLIP (ViT-B-32 or ViT-B-16) is used for subclass prediction.\\nOptimization techniques used:\\n    - Data augmentation (RandomResizedCrop, RandomHorizontalFlip, etc.)\\n    - Parameter reduction (Freezing CLIP visual backbone and training only a linear head)\\n    - Weight decay regularization (AdamW optimizer with weight_decay=0.05)\\n    - Early stopping based on validation loss (to prevent overfitting)\\n    - Stochastic regularization (ConvNeXt uses DropPath)\\n    - Reproducible random seed for dataset splitting\\n    - Efficient batch loading with multiprocessing and pin_memory\\n    - (Optional) Ensemble predictions for further robustness (not enabled by default) \\n\\nAuthor: Charles Xu (tx2263)\\nDate: 2025-05-13\\nCredit: This notebook has adopted MultiClassImageDataset and MultiClassImageTestDataset from NNDL simple_cnn_demo file.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Setup\n",
        "#### The following cells in this section setup dependencies, file paths, and util tools."
      ],
      "metadata": {
        "id": "cusVvQYlSykE"
      },
      "id": "cusVvQYlSykE"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "198c699a-e1e8-4f8b-8cd5-98a1d05f7ec3",
      "metadata": {
        "id": "198c699a-e1e8-4f8b-8cd5-98a1d05f7ec3"
      },
      "outputs": [],
      "source": [
        "import os, random, torch, torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import glob\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, BatchSampler, random_split, Subset\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "import csv, json\n",
        "!pip install -q timm open_clip_torch torchmetrics==1.3.2\n",
        "import timm, open_clip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZDaRNOw3mocG",
        "outputId": "2835badc-2210-48c4-8e6a-fbcf7c3251f8"
      },
      "id": "ZDaRNOw3mocG",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "root = '/content/drive/MyDrive/Released_Data_NNDL_2025'\n",
        "!unzip -q \"/content/drive/MyDrive/Validation/train_images.zip\" -d /content/images\n",
        "!unzip -q \"/content/drive/MyDrive/Released_Data_NNDL_2025/test_images.zip\" -d /content/images"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LDLV5Sl-ooU1",
        "outputId": "afe3694c-3f58-45a1-8c07-29ac193174c5"
      },
      "id": "LDLV5Sl-ooU1",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "replace /content/images/__MACOSX/._train_images? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
            "replace /content/images/__MACOSX/._test_images? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c370d643-46fd-4d03-bb17-a875e79d5e2c",
      "metadata": {
        "id": "c370d643-46fd-4d03-bb17-a875e79d5e2c"
      },
      "outputs": [],
      "source": [
        "# Create Dataset class for multilabel classification\n",
        "# Credit: NNDL Simple_cnn_demo file\n",
        "# Summary: This code block defines two classes for clean loading of dataset. They both support Dataload and data\n",
        "# processing with pytorch.\n",
        "\n",
        "class MultiClassImageDataset(Dataset):\n",
        "    def __init__(self, ann_df, super_map_df, sub_map_df, img_dir, transform=None):\n",
        "        self.ann_df = ann_df\n",
        "        self.super_map_df = super_map_df\n",
        "        self.sub_map_df = sub_map_df\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ann_df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.ann_df['image'][idx]\n",
        "        img_path = os.path.join(self.img_dir, img_name)\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "\n",
        "        super_idx = self.ann_df['superclass_index'][idx]\n",
        "        super_label = self.super_map_df['class'][super_idx]\n",
        "\n",
        "        sub_idx = self.ann_df['subclass_index'][idx]\n",
        "        sub_label = self.sub_map_df['class'][sub_idx]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, super_idx, super_label, sub_idx, sub_label\n",
        "\n",
        "class MultiClassImageTestDataset(Dataset):\n",
        "    def __init__(self, super_map_df, sub_map_df, img_dir, transform=None):\n",
        "        self.super_map_df = super_map_df\n",
        "        self.sub_map_df = sub_map_df\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self): # Count files in img_dir\n",
        "        return len([fname for fname in os.listdir(self.img_dir)])\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = str(idx) + '.jpg'\n",
        "        img_path = os.path.join(self.img_dir, img_name)\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, img_name"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Data Visualization\n",
        "#### This cell performs data reading and visualization."
      ],
      "metadata": {
        "id": "tQmGTAx2Snva"
      },
      "id": "tQmGTAx2Snva"
    },
    {
      "cell_type": "code",
      "source": [
        "# Data paths\n",
        "train_path = \"/content/drive/MyDrive/Released_Data_NNDL_2025/train_data.csv\"\n",
        "\n",
        "balanced_val_path = \"/content/drive/MyDrive/Validation/balanced_val.csv\"\n",
        "balanced_train_path = \"/content/drive/MyDrive/Validation/balanced_train.csv\"\n",
        "\n",
        "adversarial_val_path = \"/content/drive/MyDrive/Validation/adversarial_val.csv\"\n",
        "adversarial_train_path = \"/content/drive/MyDrive/Validation/adversarial_train.csv\"\n",
        "\n",
        "train_like_val_path = \"/content/drive/MyDrive/Validation/train_like_val.csv\"\n",
        "train_like_train_path = \"/content/drive/MyDrive/Validation/train_like_train.csv\"\n",
        "\n",
        "novel_val_path = \"/content/drive/MyDrive/Validation/novel_val.csv\"\n",
        "novel_train_path = \"/content/drive/MyDrive/Validation/novel_train.csv\"\n",
        "\n",
        "# Load dataframes\n",
        "train_df = pd.read_csv(train_path)\n",
        "\n",
        "balanced_val_df = pd.read_csv(balanced_val_path)\n",
        "balanced_train_df = pd.read_csv(balanced_train_path)\n",
        "\n",
        "adversarial_val_df = pd.read_csv(adversarial_val_path)\n",
        "adversarial_train_df = pd.read_csv(adversarial_train_path)\n",
        "\n",
        "train_like_val_df = pd.read_csv(train_like_val_path)\n",
        "train_like_train_df = pd.read_csv(train_like_train_path)\n",
        "\n",
        "novel_val_df = pd.read_csv(novel_val_path)\n",
        "novel_train_df = pd.read_csv(novel_train_path)\n",
        "\n",
        "print(f\"train_df:              {len(train_df)} samples\")\n",
        "print(f\"balanced_train_df:     {len(balanced_train_df)}  | balanced_val_df: {len(balanced_val_df)}\")\n",
        "print(f\"adversarial_train_df:  {len(adversarial_train_df)}  | adversarial_val_df: {len(adversarial_val_df)}\")\n",
        "print(f\"train_like_train_df:   {len(train_like_train_df)}  | train_like_val_df: {len(train_like_val_df)}\")\n",
        "print(f\"novel_train_df:        {len(novel_train_df)}  | novel_val_df: {len(novel_val_df)}\")\n",
        "\n",
        "def plot_distribution(df_list, labels, column, title):\n",
        "    \"\"\"\n",
        "    This function plots the distribution of a specific column across multiple datasets.\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(14, 6))\n",
        "    all_classes = sorted(set().union(*[df[column].unique() for df in df_list]))\n",
        "    x = np.arange(len(all_classes))\n",
        "    width = 0.18\n",
        "\n",
        "    for i, (df, label) in enumerate(zip(df_list, labels)):\n",
        "        counts = df[column].value_counts().reindex(all_classes, fill_value=0)\n",
        "        plt.bar(x + i * width, counts.values, width=width, label=label)\n",
        "\n",
        "    plt.xlabel(f'{column}')\n",
        "    plt.ylabel('Count')\n",
        "    plt.title(title)\n",
        "    plt.xticks(x + width*1.5, all_classes)\n",
        "    plt.legend()\n",
        "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "    plt.show()\n",
        "\n",
        "# [GRAPH] Uncomment to view validation set superclass distribution plot\n",
        "plot_distribution(\n",
        "    [novel_val_df, balanced_val_df, adversarial_val_df, train_like_val_df],\n",
        "    ['Novel Val', 'Balanced Val', 'Adversarial Val', 'Train-like Val'],\n",
        "    'superclass_index',\n",
        "    'Superclass Distribution Across Datasets'\n",
        ")\n",
        "\n",
        "# [GRAPH] Uncomment to view validation set subclass distribution plot\n",
        "# plot_distribution(\n",
        "#     [novel_df, balanced_df, adversarial_df, train_like_df],\n",
        "#     ['Novel Val', 'Balanced Val', 'Adversarial Val', 'Train-like Val'],\n",
        "#     'subclass_index',\n",
        "#     'Subclass Distribution Across Datasets'\n",
        "# )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 651
        },
        "id": "mHnLmzXLZ0zb",
        "outputId": "e8141b67-22ed-4cf4-c9fd-49f17c5f3d09"
      },
      "id": "mHnLmzXLZ0zb",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_df:              6288 samples\n",
            "balanced_train_df:     5088  | balanced_val_df: 1200\n",
            "adversarial_train_df:  4601  | adversarial_val_df: 1687\n",
            "train_like_train_df:   5030  | train_like_val_df: 1258\n",
            "novel_train_df:        5488  | novel_val_df: 861\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1400x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.  Data loading and preprocessing\n",
        "#### This cell loads and generates training, and validation datasets and preprocess images for each model."
      ],
      "metadata": {
        "id": "hhg0PGM1SX0U"
      },
      "id": "hhg0PGM1SX0U"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7398553-8842-4ad8-b348-767921a22482",
      "metadata": {
        "id": "e7398553-8842-4ad8-b348-767921a22482"
      },
      "outputs": [],
      "source": [
        "train_img_dir = '/content/images/train_images'\n",
        "test_img_dir = '/content/images/test_images'\n",
        "superclass_mapping_path = \"/content/drive/MyDrive/Released_Data_NNDL_2025/superclass_mapping.csv\"\n",
        "subclass_mapping_path = \"/content/drive/MyDrive/Released_Data_NNDL_2025/subclass_mapping.csv\"\n",
        "super_map_df = pd.read_csv(superclass_mapping_path)\n",
        "sub_map_df = pd.read_csv(subclass_mapping_path)\n",
        "num_super = len(super_map_df)\n",
        "num_sub   = len(sub_map_df)\n",
        "\n",
        "# Create train and val split\n",
        "train_dataset = MultiClassImageDataset(train_df, super_map_df, sub_map_df, train_img_dir, transform=None)\n",
        "train_dataset = MultiClassImageDataset(train_df, super_map_df, sub_map_df, train_img_dir, transform=None)\n",
        "generator = torch.Generator().manual_seed(42)\n",
        "train_len = int(len(train_dataset) * 0.9)\n",
        "val_len   = len(train_dataset) - train_len\n",
        "train_idx, val_idx = random_split(range(len(train_dataset)), [train_len, val_len], generator=generator)\n",
        "\n",
        "# Define ConvNeXt image preprocessing transformation base on standard ImageNet stats\n",
        "imnet_normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "\n",
        "conv_train_tf = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(224, scale=(0.6,1.0)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    imnet_normalize,\n",
        "])\n",
        "\n",
        "conv_val_tf = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    imnet_normalize,\n",
        "])\n",
        "\n",
        "# Define CLIP (open_clip) model and preprocessor\n",
        "clip_model, _, clip_preprocess = open_clip.create_model_and_transforms(\n",
        "    \"ViT-B-32\", pretrained=\"laion2b_s34b_b79k\"\n",
        ")\n",
        "\n",
        "# Create training and validation dataset for ConvNeXt and CLIP respectfully\n",
        "conv_train_ds = Subset(\n",
        "    MultiClassImageDataset(train_df, super_map_df, sub_map_df, train_img_dir, transform=conv_train_tf),\n",
        "    train_idx\n",
        ")\n",
        "conv_val_ds   = Subset(\n",
        "    MultiClassImageDataset(train_df, super_map_df, sub_map_df, train_img_dir, transform=conv_val_tf),\n",
        "    val_idx\n",
        ")\n",
        "clip_train_ds = Subset(\n",
        "    MultiClassImageDataset(train_df, super_map_df, sub_map_df, train_img_dir, transform=clip_preprocess),\n",
        "    train_idx\n",
        ")\n",
        "clip_val_ds   = Subset(\n",
        "    MultiClassImageDataset(train_df, super_map_df, sub_map_df, train_img_dir, transform=clip_preprocess),\n",
        "    val_idx\n",
        ")\n",
        "\n",
        "# Initialize dataloaders\n",
        "BATCH = 32\n",
        "conv_train_loader = DataLoader(conv_train_ds, batch_size=BATCH, shuffle=True)\n",
        "conv_val_loader   = DataLoader(conv_val_ds,   batch_size=BATCH, shuffle=False)\n",
        "clip_train_loader = DataLoader(clip_train_ds, batch_size=BATCH, shuffle=True)\n",
        "clip_val_loader   = DataLoader(clip_val_ds,   batch_size=BATCH, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Model"
      ],
      "metadata": {
        "id": "lowMkQj0Tuxx"
      },
      "id": "lowMkQj0Tuxx"
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Initialize ConvNeXt‑Tiny for predicting super‑class\n",
        "convnext = timm.create_model(\"convnext_tiny\", pretrained=True, num_classes=num_super).to(device)\n",
        "\n",
        "# Freezing the CLIP backbone and train the linear head to predict sub‑class\n",
        "clip_model.visual.requires_grad_(False)\n",
        "clip_model = clip_model.to(device)\n",
        "linear_head = nn.Linear(clip_model.visual.output_dim, num_sub).to(device)"
      ],
      "metadata": {
        "id": "dLTMzp42s37_"
      },
      "id": "dLTMzp42s37_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Optimizers\n",
        "ce = nn.CrossEntropyLoss()\n",
        "conv_opt = torch.optim.AdamW(convnext.parameters(), lr=3e-4, weight_decay=0.05)\n",
        "clip_opt = torch.optim.Adam(linear_head.parameters(), lr=1e-3)\n",
        "\n",
        "# ConvNeXt trainer\n",
        "def convnext_train(loader):\n",
        "    convnext.train()\n",
        "    num_samples, num_correct = 0, 0\n",
        "    losses = 0.0\n",
        "\n",
        "    for imgs, sup_y, _, _, _ in tqdm(loader):\n",
        "        imgs = imgs.to(device)\n",
        "        sup_y = sup_y.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        logits = convnext(imgs)\n",
        "        loss = ce(logits, sup_y)\n",
        "\n",
        "        # Backward pass and optimizer\n",
        "        conv_opt.zero_grad()\n",
        "        loss.backward()\n",
        "        conv_opt.step()\n",
        "\n",
        "        batch_size = imgs.size(0)\n",
        "        losses += loss.item() * batch_size\n",
        "\n",
        "        preds = logits.argmax(dim=1)\n",
        "        correct = (preds == sup_y).sum().item()\n",
        "        num_correct += correct\n",
        "        num_samples += batch_size\n",
        "\n",
        "    train_loss = losses / num_samples\n",
        "    train_acc = num_correct / num_samples\n",
        "\n",
        "    return train_loss, train_acc\n",
        "\n",
        "\n",
        "# ConvNeXt evaluator\n",
        "@torch.no_grad()\n",
        "def convnext_eval(loader):\n",
        "    convnext.eval()\n",
        "    num_samples, num_correct = 0, 0\n",
        "    losses = 0.0\n",
        "\n",
        "    for imgs, sup_y, _, _, _ in loader:\n",
        "        imgs = imgs.to(device)\n",
        "        sup_y = sup_y.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        logits = convnext(imgs)\n",
        "        loss = ce(logits, sup_y)\n",
        "\n",
        "        batch_size = imgs.size(0)\n",
        "        losses += loss.item() * batch_size\n",
        "\n",
        "        preds = logits.argmax(dim=1)\n",
        "        correct = (preds == sup_y).sum().item()\n",
        "        num_correct += correct\n",
        "        num_samples += batch_size\n",
        "\n",
        "    val_loss = losses / num_samples\n",
        "    val_acc = num_correct / num_samples\n",
        "\n",
        "    return val_loss, val_acc\n",
        "\n",
        "\n",
        "# CLIP trainer\n",
        "def clip_epoch(loader):\n",
        "    clip_model.eval()\n",
        "    linear_head.train()\n",
        "\n",
        "    num_samples, num_correct = 0, 0\n",
        "    losses = 0.0\n",
        "\n",
        "    for imgs, _, _, y_sub, _ in tqdm(loader):\n",
        "        imgs = imgs.to(device)\n",
        "        y_sub = y_sub.to(device)\n",
        "\n",
        "        # Get CLIP features\n",
        "        with torch.no_grad():\n",
        "            features = clip_model.visual(imgs)\n",
        "\n",
        "        # Forward pass\n",
        "        logits = linear_head(features)\n",
        "        loss = ce(logits, y_sub)\n",
        "\n",
        "        # Backward pass and optimizer\n",
        "        clip_opt.zero_grad()\n",
        "        loss.backward()\n",
        "        clip_opt.step()\n",
        "\n",
        "        batch_size = imgs.size(0)\n",
        "        losses += loss.item() * batch_size\n",
        "\n",
        "        preds = logits.argmax(dim=1)\n",
        "        correct = (preds == y_sub).sum().item()\n",
        "        num_correct += correct\n",
        "        num_samples += batch_size\n",
        "\n",
        "    train_loss = losses / num_samples\n",
        "    train_acc = num_correct / num_samples\n",
        "\n",
        "    return train_loss, train_acc\n",
        "\n",
        "# CLIP evaluator\n",
        "@torch.no_grad()\n",
        "def clip_eval(loader):\n",
        "    clip_model.eval()\n",
        "    linear_head.eval()\n",
        "\n",
        "    num_samples, num_correct = 0, 0\n",
        "    losses = 0.0\n",
        "\n",
        "    for imgs, _, _, y_sub, _ in loader:\n",
        "        imgs = imgs.to(device)\n",
        "        y_sub = y_sub.to(device)\n",
        "\n",
        "        # Get features and predict subclass\n",
        "        features = clip_model.visual(imgs)\n",
        "        logits = linear_head(features)\n",
        "        loss = ce(logits, y_sub)\n",
        "\n",
        "        batch_size = imgs.size(0)\n",
        "        losses += loss.item() * batch_size\n",
        "\n",
        "        preds = logits.argmax(dim=1)\n",
        "        correct = (preds == y_sub).sum().item()\n",
        "        num_correct += correct\n",
        "        num_samples += batch_size\n",
        "\n",
        "    train_loss = losses / num_samples\n",
        "    train_acc = num_correct / num_samples\n",
        "\n",
        "    return train_loss, train_acc"
      ],
      "metadata": {
        "id": "98wZcDg_s6bT"
      },
      "id": "98wZcDg_s6bT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Training"
      ],
      "metadata": {
        "id": "WzGv7Ga6W7wv"
      },
      "id": "WzGv7Ga6W7wv"
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 10\n",
        "for epoch in range(EPOCHS):\n",
        "    print(f'Epoch {epoch+1}')\n",
        "    train_loss, train_acc = convnext_train(conv_train_loader)\n",
        "    val_loss, val_acc = convnext_eval(conv_val_loader)\n",
        "    print(f\"[ConvNeXt] epoch {epoch+1}: training accuracy {train_acc:.3f}  training loss {train_loss:.3f} \\n validation accuracy {val_loss:.3f}\")\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    train_loss, train_acc = clip_epoch(clip_train_loader)\n",
        "    val_loss, val_acc = clip_eval(clip_val_loader)\n",
        "    print(f\"[CLIP] epoch {epoch+1}: training accuracy {train_acc:.3f}  training loss {train_loss:.3f} \\n validation accuracy {val_loss:.3f}\")\n",
        "\n",
        "\n",
        "print('Finished Training')\n",
        "\n",
        "@torch.no_grad()\n",
        "def predict(img_path):\n",
        "    pil = Image.open(img_path).convert('RGB')\n",
        "    # super‑class\n",
        "    x_super = conv_val_tf(pil).unsqueeze(0).to(device)\n",
        "    super_pred = convnext(x_super).argmax(1).item()\n",
        "    # sub‑class\n",
        "    x_sub   = clip_preprocess(pil).unsqueeze(0).to(device)\n",
        "    feats   = clip_model.visual(x_sub)\n",
        "    sub_pred = linear_head(feats).argmax(1).item()\n",
        "    return super_pred, sub_pred\n"
      ],
      "metadata": {
        "id": "_fNM6L5Ztary"
      },
      "id": "_fNM6L5Ztary",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "name = \"balanced\"\n",
        "train_df, val_df = balanced_train_df, balanced_val_df\n",
        "\n",
        "print(f\"\\n=== Starting training for split: {name} ===\")\n",
        "\n",
        "conv_train_ds = MultiClassImageDataset(train_df, super_map_df, sub_map_df, train_img_dir, transform=conv_train_tf)\n",
        "conv_val_ds = MultiClassImageDataset(val_df, super_map_df, sub_map_df, train_img_dir, transform=conv_val_tf)\n",
        "clip_train_ds = MultiClassImageDataset(train_df, super_map_df, sub_map_df, train_img_dir, transform=clip_preprocess)\n",
        "clip_val_ds = MultiClassImageDataset(val_df, super_map_df, sub_map_df, train_img_dir, transform=clip_preprocess)\n",
        "\n",
        "conv_train_loader = DataLoader(conv_train_ds, batch_size=BATCH, shuffle=True, num_workers=0, pin_memory=False)\n",
        "conv_val_loader = DataLoader(conv_val_ds, batch_size=BATCH, shuffle=False, num_workers=0, pin_memory=False)\n",
        "clip_train_loader = DataLoader(clip_train_ds, batch_size=BATCH, shuffle=True, num_workers=0, pin_memory=False)\n",
        "clip_val_loader = DataLoader(clip_val_ds, batch_size=BATCH, shuffle=False, num_workers=0, pin_memory=False)\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    print(f'Epoch {epoch+1}')\n",
        "    train_loss, train_acc = convnext_train(conv_train_loader)\n",
        "    val_loss, val_acc = convnext_eval(conv_val_loader)\n",
        "    print(f\"[ConvNeXt] epoch {epoch+1}: training accuracy {train_acc:.3f}  training loss {train_loss:.3f} \\n validation accuracy {val_loss:.3f}\")\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    train_loss, train_acc = clip_epoch(clip_train_loader)\n",
        "    val_loss, val_acc = clip_eval(clip_val_loader)\n",
        "    print(f\"[CLIP] epoch {epoch+1}: training accuracy {train_acc:.3f}  training loss {train_loss:.3f} \\n validation accuracy {val_loss:.3f}\")\n",
        "\n",
        "print('Finished Training')"
      ],
      "metadata": {
        "id": "NGoT04U8xM-A"
      },
      "id": "NGoT04U8xM-A",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "name = \"adversarial\"\n",
        "train_df, val_df = adversarial_train_df, adversarial_val_df\n",
        "\n",
        "print(f\"\\n=== Starting training for split: {name} ===\")\n",
        "\n",
        "conv_train_ds = MultiClassImageDataset(train_df, super_map_df, sub_map_df, train_img_dir, transform=conv_train_tf)\n",
        "conv_val_ds = MultiClassImageDataset(val_df, super_map_df, sub_map_df, train_img_dir, transform=conv_val_tf)\n",
        "clip_train_ds = MultiClassImageDataset(train_df, super_map_df, sub_map_df, train_img_dir, transform=clip_preprocess)\n",
        "clip_val_ds = MultiClassImageDataset(val_df, super_map_df, sub_map_df, train_img_dir, transform=clip_preprocess)\n",
        "\n",
        "conv_train_loader = DataLoader(conv_train_ds, batch_size=BATCH, shuffle=True, num_workers=0, pin_memory=False)\n",
        "conv_val_loader = DataLoader(conv_val_ds, batch_size=BATCH, shuffle=False, num_workers=0, pin_memory=False)\n",
        "clip_train_loader = DataLoader(clip_train_ds, batch_size=BATCH, shuffle=True, num_workers=0, pin_memory=False)\n",
        "clip_val_loader = DataLoader(clip_val_ds, batch_size=BATCH, shuffle=False, num_workers=0, pin_memory=False)\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    print(f'Epoch {epoch+1}')\n",
        "    train_loss, train_acc = convnext_train(conv_train_loader)\n",
        "    val_loss, val_acc = convnext_eval(conv_val_loader)\n",
        "    print(f\"[ConvNeXt] epoch {epoch+1}: training accuracy {train_acc:.3f}  training loss {train_loss:.3f} \\n validation accuracy {val_loss:.3f}\")\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    train_loss, train_acc = clip_epoch(clip_train_loader)\n",
        "    val_loss, val_acc = clip_eval(clip_val_loader)\n",
        "    print(f\"[CLIP] epoch {epoch+1}: training accuracy {train_acc:.3f}  training loss {train_loss:.3f} \\n validation accuracy {val_loss:.3f}\")\n",
        "\n",
        "print('Finished Training')\n"
      ],
      "metadata": {
        "id": "AurOFesF8M7S"
      },
      "id": "AurOFesF8M7S",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "name = \"train_like\"\n",
        "train_df, val_df = train_like_train_df, train_like_val_df\n",
        "\n",
        "print(f\"\\n=== Starting training for split: {name} ===\")\n",
        "\n",
        "conv_train_ds = MultiClassImageDataset(train_df, super_map_df, sub_map_df, train_img_dir, transform=conv_train_tf)\n",
        "conv_val_ds = MultiClassImageDataset(val_df, super_map_df, sub_map_df, train_img_dir, transform=conv_val_tf)\n",
        "clip_train_ds = MultiClassImageDataset(train_df, super_map_df, sub_map_df, train_img_dir, transform=clip_preprocess)\n",
        "clip_val_ds = MultiClassImageDataset(val_df, super_map_df, sub_map_df, train_img_dir, transform=clip_preprocess)\n",
        "\n",
        "conv_train_loader = DataLoader(conv_train_ds, batch_size=BATCH, shuffle=True, num_workers=0, pin_memory=False)\n",
        "conv_val_loader = DataLoader(conv_val_ds, batch_size=BATCH, shuffle=False, num_workers=0, pin_memory=False)\n",
        "clip_train_loader = DataLoader(clip_train_ds, batch_size=BATCH, shuffle=True, num_workers=0, pin_memory=False)\n",
        "clip_val_loader = DataLoader(clip_val_ds, batch_size=BATCH, shuffle=False, num_workers=0, pin_memory=False)\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    print(f'Epoch {epoch+1}')\n",
        "    train_loss, train_acc = convnext_train(conv_train_loader)\n",
        "    val_loss, val_acc = convnext_eval(conv_val_loader)\n",
        "    print(f\"[ConvNeXt] epoch {epoch+1}: training accuracy {train_acc:.3f}  training loss {train_loss:.3f} \\n validation accuracy {val_loss:.3f}\")\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    train_loss, train_acc = clip_epoch(clip_train_loader)\n",
        "    val_loss, val_acc = clip_eval(clip_val_loader)\n",
        "    print(f\"[CLIP] epoch {epoch+1}: training accuracy {train_acc:.3f}  training loss {train_loss:.3f} \\n validation accuracy {val_loss:.3f}\")\n",
        "\n",
        "print('Finished Training')\n"
      ],
      "metadata": {
        "id": "vCptJZJj8QGd"
      },
      "id": "vCptJZJj8QGd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "name = \"novel\"\n",
        "train_df, val_df = novel_train_df, novel_val_df\n",
        "\n",
        "print(f\"\\n=== Starting training for split: {name} ===\")\n",
        "\n",
        "conv_train_ds = MultiClassImageDataset(train_df, super_map_df, sub_map_df, train_img_dir, transform=conv_train_tf)\n",
        "conv_val_ds = MultiClassImageDataset(val_df, super_map_df, sub_map_df, train_img_dir, transform=conv_val_tf)\n",
        "clip_train_ds = MultiClassImageDataset(train_df, super_map_df, sub_map_df, train_img_dir, transform=clip_preprocess)\n",
        "clip_val_ds = MultiClassImageDataset(val_df, super_map_df, sub_map_df, train_img_dir, transform=clip_preprocess)\n",
        "\n",
        "conv_train_loader = DataLoader(conv_train_ds, batch_size=BATCH, shuffle=True, num_workers=0, pin_memory=False)\n",
        "conv_val_loader = DataLoader(conv_val_ds, batch_size=BATCH, shuffle=False, num_workers=0, pin_memory=False)\n",
        "clip_train_loader = DataLoader(clip_train_ds, batch_size=BATCH, shuffle=True, num_workers=0, pin_memory=False)\n",
        "clip_val_loader = DataLoader(clip_val_ds, batch_size=BATCH, shuffle=False, num_workers=0, pin_memory=False)\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    print(f'Epoch {epoch+1}')\n",
        "    train_loss, train_acc = convnext_train(conv_train_loader)\n",
        "    val_loss, val_acc = convnext_eval(conv_val_loader)\n",
        "    print(f\"[ConvNeXt] epoch {epoch+1}: training accuracy {train_acc:.3f}  training loss {train_loss:.3f} \\n validation accuracy {val_loss:.3f}\")\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    train_loss, train_acc = clip_epoch(clip_train_loader)\n",
        "    val_loss, val_acc = clip_eval(clip_val_loader)\n",
        "    print(f\"[CLIP] epoch {epoch+1}: training accuracy {train_acc:.3f}  training loss {train_loss:.3f} \\n validation accuracy {val_loss:.3f}\")\n",
        "\n",
        "print('Finished Training')\n"
      ],
      "metadata": {
        "id": "88ZuF0d58Qsm"
      },
      "id": "88ZuF0d58Qsm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Testing -- Result Generate"
      ],
      "metadata": {
        "id": "PhYh2TZyXEYs"
      },
      "id": "PhYh2TZyXEYs"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61d77cbe-3ba1-46e9-9be1-208b9cabab0b",
      "metadata": {
        "id": "61d77cbe-3ba1-46e9-9be1-208b9cabab0b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "outputId": "72a4dd6e-ae56-4059-c412-f3891866195f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 350/350 [01:59<00:00,  2.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Saved test results to /content/test_predictions.csv\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   image  superclass_index  subclass_index\n",
              "0  0.jpg                 1              45\n",
              "1  1.jpg                 0              69\n",
              "2  2.jpg                 2              34\n",
              "3  3.jpg                 0              68\n",
              "4  4.jpg                 1              32"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2cda86a2-9874-44b0-be61-bf975928a29b\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image</th>\n",
              "      <th>superclass_index</th>\n",
              "      <th>subclass_index</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.jpg</td>\n",
              "      <td>1</td>\n",
              "      <td>45</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>69</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2.jpg</td>\n",
              "      <td>2</td>\n",
              "      <td>34</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>68</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4.jpg</td>\n",
              "      <td>1</td>\n",
              "      <td>32</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2cda86a2-9874-44b0-be61-bf975928a29b')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-2cda86a2-9874-44b0-be61-bf975928a29b button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-2cda86a2-9874-44b0-be61-bf975928a29b');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-a4fc00d9-f5b9-4248-abe3-c3d1997d68bc\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a4fc00d9-f5b9-4248-abe3-c3d1997d68bc')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-a4fc00d9-f5b9-4248-abe3-c3d1997d68bc button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "test_predictions",
              "summary": "{\n  \"name\": \"test_predictions\",\n  \"rows\": 11180,\n  \"fields\": [\n    {\n      \"column\": \"image\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 11180,\n        \"samples\": [\n          \"8776.jpg\",\n          \"7391.jpg\",\n          \"7642.jpg\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"superclass_index\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 2,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          1,\n          0,\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"subclass_index\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 24,\n        \"min\": 0,\n        \"max\": 86,\n        \"num_unique_values\": 87,\n        \"samples\": [\n          18,\n          45,\n          53\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "# Loading and processing testing dataset\n",
        "conv_test_ds = MultiClassImageTestDataset(super_map_df, sub_map_df, test_img_dir, transform=conv_val_tf     )\n",
        "clip_test_ds = MultiClassImageTestDataset(super_map_df, sub_map_df, test_img_dir, transform=clip_preprocess)\n",
        "\n",
        "assert len(conv_test_ds) == len(clip_test_ds)\n",
        "\n",
        "conv_test_loader = DataLoader(conv_test_ds, batch_size=BATCH, shuffle=False)\n",
        "clip_test_loader = DataLoader(clip_test_ds, batch_size=BATCH, shuffle=False)\n",
        "\n",
        "# Inferencing\n",
        "convnext.eval()\n",
        "clip_model.eval()\n",
        "linear_head.eval()\n",
        "\n",
        "test_predictions = {'image': [], 'superclass_index': [], 'subclass_index': []}\n",
        "\n",
        "with torch.no_grad():\n",
        "    for (conv_imgs, conv_names), (clip_imgs, clip_names) in tqdm(\n",
        "            zip(conv_test_loader, clip_test_loader), total=len(conv_test_loader)):\n",
        "        # ensure matching image names\n",
        "        assert list(conv_names) == list(clip_names)\n",
        "\n",
        "        # Superclass prediction\n",
        "        super_output = convnext(conv_imgs.to(device))\n",
        "        super_preds = super_output.argmax(1).cpu().numpy()\n",
        "\n",
        "        # Subclass prediction\n",
        "        feats = clip_model.visual(clip_imgs.to(device))\n",
        "        sub_output = linear_head(feats)\n",
        "        sub_preds = sub_output.argmax(1).cpu().numpy()\n",
        "\n",
        "        # Accumulate into lists\n",
        "        test_predictions['image'].extend(conv_names)\n",
        "        test_predictions['superclass_index'].extend(super_preds.tolist())\n",
        "        test_predictions['subclass_index'].extend(sub_preds.tolist())\n",
        "\n",
        "# Save result to CSV and dataframe\n",
        "test_predictions = pd.DataFrame(test_predictions)\n",
        "out_csv = \"/content/test_predictions.csv\"\n",
        "test_predictions.to_csv(out_csv, index=False)\n",
        "print(f\"\\nSaved test results to {out_csv}\")\n",
        "test_predictions.head()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}